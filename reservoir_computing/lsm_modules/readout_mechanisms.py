"""
ðŸ§  Liquid State Machine - Neural Reservoir Computing
===================================================

ðŸ“š Research Paper:
Maass, W., NatschlÃ¤ger, T., & Markram, H. (2002)
"Real-Time Computing Without Stable States: A New Framework for Neural Computation 
Based on Perturbations"
Neural Computation, 14(11), 2531-2560

ðŸŽ¯ ELI5 Summary:
Imagine your brain as a fish tank full of tiny robots (neurons) that communicate by sending
electrical sparks to each other. When you drop a pebble (input) into the tank, it creates
ripples that bounce around between the robots. Each robot remembers what happened and 
changes how it behaves. This "liquid" of activity patterns can solve complex problems
without needing to find a stable solution - the constantly changing patterns ARE the solution!

ðŸ§ª Research Background:
Traditional neural networks require stable states and convergence to fixed points.
Maass et al. revolutionized this by showing that:
- Temporal dynamics in recurrent networks can perform universal computation
- No equilibrium states needed - perturbations drive computation
- Biological neural microcircuits naturally implement this principle
- Short-term synaptic plasticity enables rich temporal processing

ðŸ”¬ Mathematical Framework:
The LSM separates into two components:
- Liquid (L): Dynamic reservoir of spiking neurons with recurrent connectivity
- Readout (R): Maps liquid states to desired outputs

Liquid dynamics: dV/dt = -V/Ï„ + RI(t) + noise
State extraction: x(t) = {spike patterns over time window}
Readout: y(t) = f(x(t)) where f is typically linear

ðŸŽ¨ ASCII Diagram - LSM Architecture:
=====================================

    Input Stream u(t)
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LIQUID    â”‚  â† Recurrent spiking neural network
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     - 100-1000 LIF neurons
    â”‚  â”‚ â—‹â”€â”€â”€â—‹â”€â—‹ â”‚ â”‚     - ~15% connectivity  
    â”‚  â”‚ â”‚ â•² â•± â”‚ â”‚ â”‚     - Dynamic synapses
    â”‚  â”‚ â—‹â”€â—‹â”€â—‹â”€â—‹ â”‚ â”‚     - Temporal dynamics
    â”‚  â”‚  â•± â”‚ â•²  â”‚ â”‚
    â”‚  â”‚ â—‹â”€â”€â”€â—‹â”€â—‹ â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼ x(t) - Liquid States
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   READOUT   â”‚  â† Linear readout function
    â”‚  â•”â•â•â•â•â•â•â•â•—  â”‚     - Maps statesâ†’outputs
    â”‚  â•‘ WÃ—x(t) â•‘  â”‚     - Trainable weights W
    â”‚  â•šâ•â•â•â•â•â•â•â•  â”‚     - No recurrence
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    Output y(t)

ðŸ—ï¸ Implementation Features:
âœ… Multiple neuron models (LIF, Izhikevich, biological)
âœ… Dynamic synapses with short-term plasticity  
âœ… Configurable network topologies
âœ… Paper-accurate Maass 2002 parameters
âœ… Multiple readout mechanisms
âœ… Temporal pattern classification
âœ… Real-time processing capabilities

ðŸŽ›ï¸ Configuration Options:
- Neuron types: Simple LIF, Biological LIF, Izhikevich, Hodgkin-Huxley
- Synapse models: Static, Markram Dynamic, Tsodyks-Markram STP
- Connectivity: Random, Distance-dependent, Small-world, Scale-free
- State extraction: Spike counts, PSP decay, Membrane potentials
- Readout: Linear regression, Population neurons, P-delta learning

ðŸ‘¨â€ðŸ’» Author: Benedict Chen
ðŸ’° Donations: Help support this work! Buy me a coffee â˜•, beer ðŸº, or lamborghini ðŸŽï¸
   PayPal: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
   ðŸ’– Please consider recurring donations to fully support continued research

ðŸ”— Related Work: Echo State Networks, Neural Reservoir Computing, Biological Neural Networks
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Any, Union, Literal
from dataclasses import dataclass
from enum import Enum
from abc import ABC, abstractmethod
import sys
import os

# Add parent directory to path for donation_utils import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from donation_utils import show_donation_message, show_completion_message


# Configuration options for different components
class ReadoutMechanism(ABC):
    """
    Abstract base class for readout mechanisms
    
    Supports multiple approaches: linear regression, population neurons, 
    p-delta learning, perceptron, SVM, etc.
    """
    
    @abstractmethod
    def train(self, features: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """
        ðŸŽ“ Train Readout on Liquid State Features - Maass 2002 Implementation!
        
        Args:
            features: Liquid state features [n_samples, n_features]
            targets: Target outputs [n_samples, n_outputs]
            
        Returns:
            Dict containing training results and metrics
            
        ðŸ“š **Reference**: 
        "The readout consists of a population of I&F neurons trained with 
        the p-delta learning rule" - Maass et al. 2002
        
        ðŸ“ˆ **Training Progress**:
        ```python
        result = readout.train(liquid_states, targets)
        print(f"Training MSE: {result['mse']}")
        print(f"Epochs: {result['epochs']}")
        ```
        """
        pass
    
    @abstractmethod
    def predict(self, features: np.ndarray) -> np.ndarray:
        """
        ðŸ”® Generate Predictions Using Trained Readout - Real-Time Computation!
        
        Args:
            features: Liquid state features [n_samples, n_features]
            
        Returns:
            np.ndarray: Predictions [n_samples, n_outputs]
            
        ðŸš€ **Real-Time Performance**:
        - Optimized for minimal latency
        - Maintains temporal dynamics
        - Supports online adaptation
        
        ðŸ“Š **Example**:
        ```python
        predictions = readout.predict(current_liquid_state)
        confidence = np.max(predictions, axis=1)
        ```
        """
        pass
    
    @abstractmethod
    def reset(self):
        """Reset readout to untrained state"""
        pass


class LinearReadout(ReadoutMechanism):
    """
    Linear regression readout (current implementation)
    
    Fast and effective for many tasks, but not biologically realistic
    """
    
    def __init__(self, regularization: str = 'ridge', alpha: float = 1.0):
        self.regularization = regularization
        self.alpha = alpha
        self.readout_model = None
        
    def train(self, features: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """Train linear readout"""
        if self.regularization == 'ridge':
            from sklearn.linear_model import Ridge
            self.readout_model = Ridge(alpha=self.alpha)
        elif self.regularization == 'lasso':
            from sklearn.linear_model import Lasso
            self.readout_model = Lasso(alpha=self.alpha)
        elif self.regularization == 'none':
            from sklearn.linear_model import LinearRegression  
            self.readout_model = LinearRegression()
        else:
            raise ValueError(f"Unknown regularization: {self.regularization}")
            
        # Train readout
        self.readout_model.fit(features, targets)
        
        # Calculate performance
        predictions = self.readout_model.predict(features)
        mse = np.mean((predictions - targets) ** 2)
        
        results = {
            'mse': mse,
            'n_features': features.shape[1],
            'readout_method': f'linear_{self.regularization}',
            'regularization': self.alpha
        }
        
        return results
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """Generate predictions"""
        if self.readout_model is None:
            raise ValueError("Readout must be trained before prediction!")
        return self.readout_model.predict(features)
    
    def reset(self):
        """Reset to untrained state"""
        self.readout_model = None


class PopulationReadout(ReadoutMechanism):
    """
    Population of I&F readout neurons with biologically realistic dynamics
    
    Addresses FIXME: Missing population readout neurons from Maass 2002
    """
    
    def __init__(self, n_readout: int = 10, learning_rate: float = 0.01):
        self.n_readout = n_readout
        self.learning_rate = learning_rate
        
        # Initialize readout neurons (using configurable LIF neurons)
        neuron_config = LIFNeuronConfig(model_type=NeuronModelType.MAASS_2002_LIF)
        self.readout_neurons = [LIFNeuron(neuron_config, neuron_type='E') for _ in range(n_readout)]
        
        # Connection weights (liquid -> readout)
        self.W_readout = None
        self.trained = False
        
    def train(self, features: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """
        Train population readout with supervised learning
        
        Simplified version of p-delta learning from Maass 2002
        """
        n_samples, n_features = features.shape
        
        # Initialize weights if not done
        if self.W_readout is None:
            self.W_readout = np.random.normal(0, 0.1, (self.n_readout, n_features))
        
        # Training loop (simplified p-delta algorithm)
        n_epochs = 100
        for epoch in range(n_epochs):
            total_error = 0
            
            for i in range(n_samples):
                liquid_state = features[i]
                target = targets[i] if np.isscalar(targets[i]) else targets[i][0]
                
                # Forward pass through population
                readout_activity = np.zeros(self.n_readout)
                for j in range(self.n_readout):
                    # Compute input current
                    input_current = np.sum(self.W_readout[j] * liquid_state)
                    
                    # Simple rate-based approximation (could be replaced with spiking dynamics)
                    readout_activity[j] = max(0, input_current)  # ReLU activation
                
                # Population response (mean firing rate)
                population_response = np.mean(readout_activity)
                
                # Error signal
                error = target - population_response
                total_error += error ** 2
                
                # Weight update (gradient descent)
                for j in range(self.n_readout):
                    self.W_readout[j] += self.learning_rate * error * liquid_state
        
        self.trained = True
        
        # Calculate final performance
        predictions = self.predict(features)
        mse = np.mean((predictions - targets) ** 2)
        
        results = {
            'mse': mse,
            'n_features': n_features,
            'readout_method': 'population_neurons',
            'n_readout_neurons': self.n_readout,
            'learning_rate': self.learning_rate
        }
        
        return results
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """Generate predictions using population response"""
        if not self.trained or self.W_readout is None:
            raise ValueError("Readout must be trained before prediction!")
        
        predictions = []
        
        for i in range(features.shape[0]):
            liquid_state = features[i]
            
            # Compute population activity
            readout_activity = np.zeros(self.n_readout)
            for j in range(self.n_readout):
                input_current = np.sum(self.W_readout[j] * liquid_state)
                readout_activity[j] = max(0, input_current)  # ReLU activation
            
            # Population response (mean activity)
            population_response = np.mean(readout_activity)
            predictions.append(population_response)
        
        return np.array(predictions)
    
    def reset(self):
        """Reset to untrained state"""
        self.W_readout = None
        self.trained = False
        
        # Reset readout neurons
        for neuron in self.readout_neurons:
            # Reset neuron state (simplified)
            neuron.v_membrane = neuron.config.v_rest
            neuron.refractory_time = 0.0


class PerceptronReadout(ReadoutMechanism):
    """
    Simple perceptron readout for binary classification
    
    Biologically plausible single-layer learning
    """
    
    def __init__(self, learning_rate: float = 0.1, max_epochs: int = 1000):
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        self.weights = None
        self.bias = None
        self.trained = False
        
    def train(self, features: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """Train perceptron with simple learning rule"""
        n_samples, n_features = features.shape
        
        # Initialize weights and bias
        self.weights = np.random.normal(0, 0.01, n_features)
        self.bias = 0.0
        
        # Convert targets to binary (-1, +1)
        binary_targets = np.where(targets > np.mean(targets), 1, -1)
        
        # Training loop
        for epoch in range(self.max_epochs):
            n_errors = 0
            
            for i in range(n_samples):
                # Forward pass
                activation = np.dot(features[i], self.weights) + self.bias
                prediction = 1 if activation >= 0 else -1
                
                # Update if error
                if prediction != binary_targets[i]:
                    n_errors += 1
                    error = binary_targets[i] - prediction
                    
                    # Perceptron learning rule
                    self.weights += self.learning_rate * error * features[i]
                    self.bias += self.learning_rate * error
            
            # Early stopping if converged
            if n_errors == 0:
                break
        
        self.trained = True
        
        # Calculate performance
        predictions = self.predict(features)
        accuracy = np.mean((predictions > 0) == (targets > np.mean(targets)))
        
        results = {
            'accuracy': accuracy,
            'n_features': n_features,
            'readout_method': 'perceptron',
            'epochs_trained': epoch + 1,
            'learning_rate': self.learning_rate
        }
        
        return results
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """Generate predictions"""
        if not self.trained or self.weights is None:
            raise ValueError("Readout must be trained before prediction!")
        
        activations = np.dot(features, self.weights) + self.bias
        return activations  # Return raw activations (can be thresholded externally)
    
    def reset(self):
        """Reset to untrained state"""
        self.weights = None
        self.bias = None
        self.trained = False


class SVMReadout(ReadoutMechanism):
    """
    Support Vector Machine readout
    
    High-performance nonlinear classification/regression
    """
    
    def __init__(self, kernel: str = 'rbf', C: float = 1.0, gamma: str = 'scale'):
        self.kernel = kernel
        self.C = C
        self.gamma = gamma
        self.svm_model = None
        
    def train(self, features: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """Train SVM readout"""
        try:
            from sklearn.svm import SVR, SVC
        except ImportError:
            raise ImportError("scikit-learn required for SVM readout")
        
        # Determine if classification or regression
        n_unique_targets = len(np.unique(targets))
        if n_unique_targets <= 10:  # Assume classification
            self.svm_model = SVC(kernel=self.kernel, C=self.C, gamma=self.gamma)
        else:  # Regression
            self.svm_model = SVR(kernel=self.kernel, C=self.C, gamma=self.gamma)
        
        # Train model
        self.svm_model.fit(features, targets)
        
        # Calculate performance
        predictions = self.svm_model.predict(features)
        if hasattr(self.svm_model, 'score'):
            score = self.svm_model.score(features, targets)
        else:
            score = np.mean((predictions - targets) ** 2)  # MSE for regression
        
        results = {
            'score': score,
            'n_features': features.shape[1],
            'readout_method': f'svm_{self.kernel}',
            'kernel': self.kernel,
            'C': self.C
        }
        
        return results
    
    def predict(self, features: np.ndarray) -> np.ndarray:
        """Generate predictions"""
        if self.svm_model is None:
            raise ValueError("Readout must be trained before prediction!")
        return self.svm_model.predict(features)
    
    def reset(self):
        """Reset to untrained state"""
        self.svm_model = None


class PopulationReadoutNeurons:
    """
    Population of integrate-and-fire readout neurons with p-delta learning rule.
    Current linear readout completely misses biological realism of paper.
    
    From Maass 2002: "The readout consists of a population of I&F neurons
    trained with the p-delta learning rule"
    """
    def __init__(self, n_readout=10, n_liquid=160):
        self.n_readout = n_readout
        self.n_liquid = n_liquid
        
        # Initialize readout neurons
        self.readout_neurons = [BiologicalLIFNeuron() for _ in range(n_readout)]
        
        # Initialize connection weights (liquid -> readout)
        self.W_readout = np.random.normal(0, 0.1, (n_readout, n_liquid))
        
        # p-delta learning parameters
        self.learning_rate = 0.01
        self.eligibility_traces = np.zeros((n_readout, n_liquid))
        self.trace_decay = 0.99
        
    def forward(self, liquid_states, dt):
        """Forward pass through readout population"""
        n_timesteps = liquid_states.shape[0]
        readout_spikes = np.zeros((self.n_readout, n_timesteps))
        
        for t in range(n_timesteps):
            liquid_state = liquid_states[t]
            
            for i, neuron in enumerate(self.readout_neurons):
                # Compute synaptic input
                synaptic_input = np.sum(self.W_readout[i] * liquid_state)
                
                # Update neuron
                spike = neuron.update(dt, spike_inputs_exc=synaptic_input)
                readout_spikes[i, t] = 1.0 if spike else 0.0
                
        return readout_spikes
    
    def train_p_delta(self, liquid_states, target_spikes, dt):
        """Train with p-delta learning rule from Maass 2002"""
        # FIXME: Implement full p-delta algorithm
        # This is a simplified version - full implementation needed
        
        readout_spikes = self.forward(liquid_states, dt)
        
        for t in range(liquid_states.shape[0]):
            liquid_state = liquid_states[t]
            
            for i in range(self.n_readout):
                # Update eligibility traces
                self.eligibility_traces[i] *= self.trace_decay
                self.eligibility_traces[i] += liquid_state
                
                # Compute error signal
                error = target_spikes[i, t] - readout_spikes[i, t]
                
                # Weight update
                self.W_readout[i] += self.learning_rate * error * self.eligibility_traces[i]


# FIXME: Implement benchmark tasks from Maass 2002 paper
